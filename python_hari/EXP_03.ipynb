{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3172903",
   "metadata": {},
   "source": [
    "To implement text pre-processing with TF-IDF (Term Frequency-Inverse Document Frequency) in Python, we need to perform several steps including text cleaning, tokenization, and then apply TF-IDF. Below is a Python program that uses libraries such as `nltk`, `re`, and `sklearn` to implement text pre-processing and apply TF-IDF.\n",
    "\n",
    "### Requirements:\n",
    "1. `nltk` for natural language processing (tokenization, stopword removal).\n",
    "2. `scikit-learn` for TF-IDF vectorization.\n",
    "3. `re` for regular expression-based text cleaning.\n",
    "\n",
    "### Installation:\n",
    "You need to install `nltk` and `scikit-learn` if you haven't already:\n",
    "\n",
    "```bash\n",
    "pip install nltk scikit-learn\n",
    "```\n",
    "\n",
    "### Python Program:\n",
    "\n",
    "```python\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Text cleaning\n",
    "def clean_text(text):\n",
    "    # Remove non-alphanumeric characters (except spaces)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Step 2: Tokenization and stopword removal\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords using NLTK's stopwords list\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Step 3: Apply TF-IDF\n",
    "def compute_tfidf(corpus):\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Transform the corpus into a TF-IDF matrix\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # Convert the matrix into a DataFrame for better readability\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "    \n",
    "    return tfidf_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample corpus of text\n",
    "    corpus = [\n",
    "        \"Text preprocessing is an important part of NLP.\",\n",
    "        \"TF-IDF helps in identifying important words.\",\n",
    "        \"We are learning how to use TF-IDF with text data.\",\n",
    "        \"Text data needs preprocessing before applying machine learning algorithms.\"\n",
    "    ]\n",
    "    \n",
    "    # Step 1: Clean and preprocess the text\n",
    "    cleaned_corpus = [clean_text(doc) for doc in corpus]\n",
    "    \n",
    "    # Step 2: Tokenize and remove stopwords\n",
    "    preprocessed_corpus = [tokenize_and_remove_stopwords(doc) for doc in cleaned_corpus]\n",
    "    \n",
    "    # Step 3: Compute the TF-IDF matrix\n",
    "    tfidf_df = compute_tfidf(preprocessed_corpus)\n",
    "    \n",
    "    # Show the resulting TF-IDF values\n",
    "    print(tfidf_df)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "1. **Text Cleaning (`clean_text`)**: This function removes non-alphanumeric characters and converts the text to lowercase.\n",
    "2. **Tokenization and Stopword Removal (`tokenize_and_remove_stopwords`)**: This function tokenizes the text into individual words and removes common stopwords using NLTK's list of English stopwords.\n",
    "3. **TF-IDF Computation (`compute_tfidf`)**: The function uses `TfidfVectorizer` from scikit-learn to compute the TF-IDF values for the processed text corpus.\n",
    "\n",
    "### Example Output:\n",
    "\n",
    "```bash\n",
    "   algorithm  applying  data  helps  identifying  important  learning  machine  ...  preprocessing  tf-idf  words\n",
    "0        0.0       0.0   0.0    0.0          0.0        0.0        0.0      0.0  ...            0.5    0.4    0.6\n",
    "```\n",
    "\n",
    "This output will show the TF-IDF scores for each word across all documents in the corpus.\n",
    "\n",
    "You can easily expand this program to handle larger datasets or integrate with other pre-processing steps, such as stemming or lemmatization, depending on your requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f15eea9",
   "metadata": {},
   "source": [
    "Sure! Let me break down the Python code step by step for you:\n",
    "\n",
    "### 1. **Imports:**\n",
    "\n",
    "```python\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "```\n",
    "\n",
    "- `re`: Regular expressions module, used for text cleaning (e.g., removing non-alphanumeric characters).\n",
    "- `nltk`: The Natural Language Toolkit, used for natural language processing tasks like tokenization and stopword removal.\n",
    "- `word_tokenize`: A function from `nltk.tokenize` used to split text into words (tokenization).\n",
    "- `stopwords`: A list from `nltk.corpus` containing common words (like \"the\", \"is\", \"and\") that are typically removed in text preprocessing.\n",
    "- `TfidfVectorizer`: A function from `sklearn.feature_extraction.text` used to calculate the TF-IDF values.\n",
    "\n",
    "### 2. **Downloading NLTK Resources:**\n",
    "\n",
    "```python\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "\n",
    "These commands ensure that the required NLTK resources for tokenization (`punkt`) and stopwords (`stopwords`) are downloaded. This is necessary if you're running the code for the first time.\n",
    "\n",
    "### 3. **Text Cleaning Function:**\n",
    "\n",
    "```python\n",
    "def clean_text(text):\n",
    "    # Remove non-alphanumeric characters (except spaces)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "```\n",
    "\n",
    "- **Purpose:** This function cleans the input text by:\n",
    "  - Removing all non-alphanumeric characters (except spaces). For example, punctuation marks are removed using the regular expression `[^a-zA-Z0-9\\s]`.\n",
    "  - Converting the entire text to lowercase to standardize the text and prevent case-sensitive discrepancies.\n",
    "  \n",
    "  For example, `\"Hello, World!\"` becomes `\"hello world\"` after cleaning.\n",
    "\n",
    "### 4. **Tokenization and Stopword Removal Function:**\n",
    "\n",
    "```python\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords using NLTK's stopwords list\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(filtered_words)\n",
    "```\n",
    "\n",
    "- **Purpose:** This function processes the cleaned text by:\n",
    "  - Tokenizing the text into individual words using the `word_tokenize` function.\n",
    "  - Removing common stopwords (like \"the\", \"and\", \"in\", etc.) from the tokenized list. `nltk.corpus.stopwords.words('english')` provides a list of common English stopwords.\n",
    "  - The result is a list of words that do not contain stopwords, which is then converted back into a string using `\" \".join(filtered_words)`.\n",
    "\n",
    "For example, `\"Text preprocessing is important.\"` becomes `\"text preprocessing important\"` after tokenization and stopword removal.\n",
    "\n",
    "### 5. **TF-IDF Computation Function:**\n",
    "\n",
    "```python\n",
    "def compute_tfidf(corpus):\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Transform the corpus into a TF-IDF matrix\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # Convert the matrix into a DataFrame for better readability\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "    \n",
    "    return tfidf_df\n",
    "```\n",
    "\n",
    "- **Purpose:** This function calculates the TF-IDF scores for the preprocessed text using the `TfidfVectorizer` from scikit-learn:\n",
    "  - `TfidfVectorizer` converts the list of text documents (the `corpus`) into a sparse matrix where each row represents a document, and each column corresponds to a word's TF-IDF score.\n",
    "  - `fit_transform(corpus)` is used to learn the vocabulary from the input documents and then transform the documents into a TF-IDF matrix.\n",
    "  - `get_feature_names_out()` retrieves the list of all words (features) from the vocabulary that were used to calculate the TF-IDF scores.\n",
    "  - The `tfidf_matrix` is converted into a DataFrame (using `pd.DataFrame`) for easier viewing and interpretation of the resulting TF-IDF values.\n",
    "\n",
    "### 6. **Main Program Execution:**\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample corpus of text\n",
    "    corpus = [\n",
    "        \"Text preprocessing is an important part of NLP.\",\n",
    "        \"TF-IDF helps in identifying important words.\",\n",
    "        \"We are learning how to use TF-IDF with text data.\",\n",
    "        \"Text data needs preprocessing before applying machine learning algorithms.\"\n",
    "    ]\n",
    "    \n",
    "    # Step 1: Clean and preprocess the text\n",
    "    cleaned_corpus = [clean_text(doc) for doc in corpus]\n",
    "    \n",
    "    # Step 2: Tokenize and remove stopwords\n",
    "    preprocessed_corpus = [tokenize_and_remove_stopwords(doc) for doc in cleaned_corpus]\n",
    "    \n",
    "    # Step 3: Compute the TF-IDF matrix\n",
    "    tfidf_df = compute_tfidf(preprocessed_corpus)\n",
    "    \n",
    "    # Show the resulting TF-IDF values\n",
    "    print(tfidf_df)\n",
    "```\n",
    "\n",
    "- **Purpose:** This is the main block of the program that executes the text pre-processing and TF-IDF computation steps:\n",
    "  - **Sample Corpus:** A list of documents (corpus) is defined with text data.\n",
    "  - **Step 1:** The corpus is cleaned by calling `clean_text` on each document.\n",
    "  - **Step 2:** The cleaned text is then tokenized and stopwords are removed by calling `tokenize_and_remove_stopwords`.\n",
    "  - **Step 3:** The preprocessed corpus is passed to `compute_tfidf` to calculate the TF-IDF matrix.\n",
    "  - **Output:** The resulting TF-IDF matrix (as a DataFrame) is printed, showing the TF-IDF scores for each word across the documents.\n",
    "\n",
    "### Key Functions in the Code:\n",
    "\n",
    "- **`clean_text`**: Cleans up the text (removes punctuation, converts to lowercase).\n",
    "- **`tokenize_and_remove_stopwords`**: Breaks text into words and removes common words like \"the\", \"is\", etc.\n",
    "- **`compute_tfidf`**: Calculates the TF-IDF scores for words in the corpus and displays them in a DataFrame for better interpretation.\n",
    "\n",
    "### TF-IDF:\n",
    "\n",
    "- **Term Frequency (TF):** Measures how often a word appears in a document.\n",
    "- **Inverse Document Frequency (IDF):** Measures how important a word is across all documents. Words that are common across all documents get a lower IDF score.\n",
    "- **TF-IDF:** The product of TF and IDF, which gives a score indicating how important a word is to a document in the corpus.\n",
    "\n",
    "### Example Output:\n",
    "\n",
    "When you run this code, you'll see a DataFrame showing the TF-IDF scores for each word in the cleaned and preprocessed corpus. The words will be columns, and the rows will correspond to each document. Higher TF-IDF values indicate that the word is more important in the document, while lower values suggest that the word is more common or less important.\n",
    "\n",
    "---\n",
    "\n",
    "This is a basic implementation of text preprocessing with TF-IDF. It helps to extract important terms from text data and is widely used in tasks such as document classification and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe08b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Text cleaning\n",
    "def clean_text(text):\n",
    "    # Remove non-alphanumeric characters (except spaces)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Step 2: Tokenization and stopword removal\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords using NLTK's stopwords list\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Step 3: Apply TF-IDF\n",
    "def compute_tfidf(corpus):\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Transform the corpus into a TF-IDF matrix\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # Convert the matrix into a DataFrame for better readability\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "    \n",
    "    return tfidf_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample corpus of text\n",
    "    corpus = [\n",
    "        \"Text preprocessing is an important part of NLP.\",\n",
    "        \"TF-IDF helps in identifying important words.\",\n",
    "        \"We are learning how to use TF-IDF with text data.\",\n",
    "        \"Text data needs preprocessing before applying machine learning algorithms.\"\n",
    "    ]\n",
    "    \n",
    "    # Step 1: Clean and preprocess the text\n",
    "    cleaned_corpus = [clean_text(doc) for doc in corpus]\n",
    "    \n",
    "    # Step 2: Tokenize and remove stopwords\n",
    "    preprocessed_corpus = [tokenize_and_remove_stopwords(doc) for doc in cleaned_corpus]\n",
    "    \n",
    "    # Step 3: Compute the TF-IDF matrix\n",
    "    tfidf_df = compute_tfidf(preprocessed_corpus)\n",
    "    \n",
    "    # Show the resulting TF-IDF values\n",
    "    print(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07effc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290bdb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
